{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86ce25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nLookBackDays=5\n",
    "nForecastDays=1\n",
    "hoursPerDay = 24\n",
    "nForecastHours = nForecastDays * hoursPerDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40b91d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 추가 패키지\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0423169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import pandas as pd\n",
    "#pdDataset = pd.read_excel('../1) 한전 전력 전처리/3B)HourlyPower(SeparateDate)(Day1Start)(NoWeekend).xlsx', \n",
    "#                          sheet_name = 'data')\n",
    "pdDataset = pd.read_excel('3A)HourlyPower(SeparateDate)(AnyDayStart)(NoWeekend).xlsx', \n",
    "                          sheet_name = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa424a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = pdDataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c619f01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days in total :  181\n"
     ]
    }
   ],
   "source": [
    "# 필요한 상수 정의하기\n",
    "numDaysTotal = int(datalen/hoursPerDay)\n",
    "print(\"Number of days in total : \", numDaysTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ccc9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "perDayTotalPowerList = [] # 하루 24개 데이터를 리스트 형태로 저장(총전력)\n",
    "perDayTotalPower = [] # 하루 24시간 총 전력을 하나의 값으로 해서 저장\n",
    "perDayPeakPowerList = [] # 하루 24개 데이터를 리스트 형태로 저장(peak전력)\n",
    "perDayPeakPower = [] # 하루 중 최대 전력 하나를 골라서 저장\n",
    "perDayDateInfo = [] # 년.월.일 정보를 저장\n",
    "\n",
    "for nthDay in range(numDaysTotal):\n",
    "    currHourTotalPowerList, currHourPeakPowerList = [], []\n",
    "    \n",
    "    currDate = str(pdDataset[\"Year\"][nthDay*hoursPerDay]) + \".\" \\\n",
    "               + str(pdDataset[\"Month\"][nthDay*hoursPerDay]) + \".\" \\\n",
    "               + str(pdDataset[\"Day\"][nthDay*hoursPerDay])\n",
    "    perDayDateInfo.append(currDate)\n",
    "    \n",
    "    for hour in range(hoursPerDay):\n",
    "        index = nthDay*hoursPerDay + hour\n",
    "        currHourTotalPowerList.append(pdDataset[\"TotalPower(kWh)\"][index])\n",
    "        currHourPeakPowerList.append(pdDataset[\"PeakPower(kW)\"][index])\n",
    "   \n",
    "    perDayTotalPowerList.append(currHourTotalPowerList)\n",
    "    perDayTotalPower.append(sum(currHourTotalPowerList))\n",
    "    perDayPeakPowerList.append(currHourPeakPowerList)\n",
    "    perDayPeakPower.append(max(currHourPeakPowerList))\n",
    "    \n",
    "perDayTotalPowerList = np.array(perDayTotalPowerList)\n",
    "perDayTotalPower = np.array(perDayTotalPower)\n",
    "perDayPeakPowerList = np.array(perDayPeakPowerList)\n",
    "perDayPeakPower = np.array(perDayPeakPower)\n",
    "perDayDateInfo = np.array(perDayDateInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54040b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607c3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 171 days out of 181 days\n",
      "Forecast for 10 days out of 181 days.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습에 사용할 샘플 수 정하기\n",
    "터미널 출력 결과를 보고 비중(portion)을 적절히 골랐음\n",
    "\"\"\"\n",
    "portion = 0.945\n",
    "howManyDaysToTrain = int(numDaysTotal * portion)\n",
    "\n",
    "if True:\n",
    "    print(\"Train for \" + str(howManyDaysToTrain) \n",
    "          + \" days out of \" + str(numDaysTotal) + \" days\")\n",
    "    print(\"Forecast for %d days out of %d days.\" % (numDaysTotal\n",
    "                                                    - howManyDaysToTrain, numDaysTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536c884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyUtils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637d68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHourlyPeakPower = pdDataset[\"PeakPower(kW)\"].values\n",
    "hourlyPeakPowerTrain, hourlyPeakPowerTest \\\n",
    "= split_dataset(rawHourlyPeakPower, howManyDaysToTrain, hoursPerDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aca4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyUtils import to_supervisedDaily # 1-24시간 단위로 분할\n",
    "from MyUtils import to_supervisedContinuousHours # 연속된 모든 24시간 단위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f16f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "XHourlyPeakPowerTrain, yHourlyPeakPowerTrain \\\n",
    "= to_supervisedDaily(hourlyPeakPowerTrain, nLookBackDays, nForecastDays)\n",
    "\n",
    "# 검증은 1-24시간 단위의 데이터로 할 것임 >> to_supervisedDaily 함수를 사용\n",
    "XHourlyPeakPowerTest, yHourlyPeakPowerTest \\\n",
    "= to_supervisedDaily(hourlyPeakPowerTest, nLookBackDays, nForecastDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708d53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측할 24시간에 대한 전력 peak를 미리 계산하고, 이를 조건부 입력으로 사용\n",
    "XContinuousDailyPeakPowerTrain = []\n",
    "#print(XContinuousDailyPeakPowerTrain.shape)\n",
    "\n",
    "numSamples = yHourlyPeakPowerTrain.shape[0]\n",
    "for i in range(numSamples):\n",
    "    sample = yHourlyPeakPowerTrain[i]\n",
    "    XContinuousDailyPeakPowerTrain.append(max(sample))  # 최대값을 리턴\n",
    "    if i == -1:\n",
    "        print(sample)\n",
    "        print(max(sample))\n",
    "        #break\n",
    "\n",
    "XContinuousDailyPeakPowerTrain = np.array(XContinuousDailyPeakPowerTrain)\n",
    "#print(XContinuousDailyPeakPowerTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90d7c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측할 24시간에 대한 전력 peak를 미리 계산하고, 이를 조건부 입력으로 사용\n",
    "XContinuousDailyPeakPowerTest = []\n",
    "#print(XContinuousDailyPeakPowerTest.shape)\n",
    "\n",
    "numSamples = yHourlyPeakPowerTest.shape[0]\n",
    "for i in range(numSamples):\n",
    "    sample = yHourlyPeakPowerTest[i]\n",
    "    XContinuousDailyPeakPowerTest.append(max(sample))  # 최대값을 리턴\n",
    "    if i == -1:\n",
    "        print(sample)\n",
    "        print(max(sample))\n",
    "        #break\n",
    "\n",
    "XContinuousDailyPeakPowerTest = np.array(XContinuousDailyPeakPowerTest)\n",
    "#print(XContinuousDailyPeakPowerTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c6a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from MyModels import *\n",
    "#from ForcingMaxUtils import build_transformer_conditionalInput_maxOutput_model_gAvgPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5974c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_conditionalInput_maxOutput_model_gAvgPooling(\n",
    "    # 전력 PEAK를 조건부 입력으로 제공 + 출력층에서 시계열 데이터의 PEAK를 출력\n",
    "    input_shape, conditional_input_shape, num_outputs,\n",
    "    head_size, num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0, mlp_dropout=0,\n",
    "    ):\n",
    "    \n",
    "    LayerNormEps = 1e-6\n",
    "    \"\"\"\n",
    "    인코더 \n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name=\"ts_input\") # 시계열 학습 데이터\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    # PEAK는 최대값을 예측하는 문제니까, max pooling이 더 좋으려나? 그렇지 않음.\n",
    "    #x = layers.GlobalMaxPooling1D(data_format=\"channels_first\")(x)\n",
    "    #x = layers.MaxPooling1D(pool_size=4, strides=1, padding='same', data_format=\"channels_first\")(x)\n",
    "    \n",
    "    # 조건부 입력(일간 총 사용량 예측치) 데이터를 추가로 입력 받음\n",
    "    conditional_inputs = keras.Input(shape=conditional_input_shape, name=\"ts_max_input\")\n",
    "    # 조건부 입력(일간 총 사용량)의 단위가 크고, 다른 특징들은 이미 정규화가 되어있어서\n",
    "    # 조건부 입력을 그대로 사용하면 이로 인한 업데이트가 너무나 과도해 질 수 있음\n",
    "    # 따라서, 조건부 입력에 대해서도 정규화를 실시함\n",
    "    normalized_conditional_inputs \\\n",
    "    = layers.LayerNormalization(epsilon=LayerNormEps)(conditional_inputs)\n",
    "    # 인코더의 최종 출력 + 조건부 입력을 인코더 계층의 최종 출력으로 하고, 디코더로 전달\n",
    "    x = x + normalized_conditional_inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    디코더\n",
    "    \"\"\"\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\",\n",
    "                         kernel_initializer='random_normal',\n",
    "                         bias_initializer='zeros')(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    \n",
    "    # 양수만 나와야 하니까, linear 대신 relu\n",
    "    timeseries_outputs = layers.Dense(num_outputs, activation=\"relu\",\n",
    "                                      kernel_initializer='random_normal',\n",
    "                                      bias_initializer='zeros',\n",
    "                                      name='ts_output')(x) \n",
    "    \n",
    "    #시계열 출력을 다시 입력으로 받아서, 출력의 element-wise max을 계산\n",
    "    max_outputs = layers.Lambda(lambda v: K.max(v), \n",
    "                                output_shape=(1,1),\n",
    "                                name=\"ts_max\")(timeseries_outputs)\n",
    "    # 최종 모델을 리턴\n",
    "    return keras.Model(inputs = [inputs, conditional_inputs], \n",
    "                       outputs = [timeseries_outputs, max_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f687aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from MyMetrics import mape, smape, rmse, mae, coeff_determination\n",
    "import os\n",
    "from MyUtils import plotTrainingProgress\n",
    "from ForcingMaxUtils import test_predict_mergedInput_mergedOutput_forcingMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb3167",
   "metadata": {},
   "source": [
    "## PEAK 전력 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5bcd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" # second gpu for Case 2 code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f516a2",
   "metadata": {},
   "source": [
    "### PEAK : Case 2 (Trial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1e0f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_peakpower_case2_1 = build_transformer_conditionalInput_maxOutput_model_gAvgPooling(\n",
    "    input_shape = XHourlyPeakPowerTrain.shape[1:],\n",
    "    conditional_input_shape = XContinuousDailyPeakPowerTrain.shape[1:],\n",
    "    num_outputs = nForecastHours,\n",
    "    head_size=512,\n",
    "    num_heads=8,\n",
    "    ff_dim=8, # 8 : 성능이 오히려 떨어짐\n",
    "    num_transformer_blocks=8,\n",
    "    #mlp_units=[128, 256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_units=[512, 256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_dropout=0.1, # 0.4\n",
    "    dropout=0.1) # 0.25\n",
    "\"\"\"\n",
    "시계열 max가  예측 peak와 같아지는 것을 강제하는 가중치\n",
    "peak 예측도 중요하고, 시계열 예측도 중요(몇시에 발생하는지)하므로\n",
    "peak 예측에 대한 가중치를 높여나가면서 실험 함\n",
    "\"\"\"\n",
    "ts_max_weight = 0.1\n",
    "lossWeight = {\"ts_output\":1.0-ts_max_weight,\"ts_max\":ts_max_weight}\n",
    "\n",
    "# ts : time-series\n",
    "# ts_max의 영향을 줄이기 위해서 가중치 작게 설정했고, mae를 사용함\n",
    "# => mse로 변경\n",
    "# Adam + decay를 사용할거라면, 초기 lr은 큰 값으로 설정해도 되겠다.\n",
    "#model_peakpower_case2_1.compile(loss = {\"ts_output\" : 'mse', \"ts_max\" : 'mae'},\n",
    "\"\"\"\n",
    "MyMetrics  라는 파일에 저장된 손실 함수를 사용하는 코드인데, 잘못된 구현이 있는것 같아서\n",
    "KERAS의 공식 손실 함수를 사용함\n",
    "model_peakpower_case2_1.compile(loss = {\"ts_output\" : 'mse', \"ts_max\" : 'mae'},\n",
    "                              optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                              metrics = {\"ts_output\":mae,\"ts_max\":mae},\n",
    "                              loss_weights = lossWeight)\n",
    "\"\"\"\n",
    "model_peakpower_case2_1.compile(loss = {\"ts_output\" : 'mean_squared_error', \"ts_max\" : 'mean_absolute_error'},\n",
    "                                optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                                metrics = {\"ts_output\" : 'mean_absolute_error',\"ts_max\" : 'mean_absolute_error'},\n",
    "                                loss_weights = lossWeight)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True)]\n",
    "\n",
    "training_history_peakpower_case2_1 = \\\n",
    "model_peakpower_case2_1.fit(\n",
    "    x = {\"ts_input\" : XHourlyPeakPowerTrain, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTrain},\n",
    "    y = {\"ts_output\" : yHourlyPeakPowerTrain, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTrain},    \n",
    "    validation_data=(\n",
    "        {\"ts_input\" : XHourlyPeakPowerTest, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTest},\n",
    "        {\"ts_output\" : yHourlyPeakPowerTest, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTest},        \n",
    "    ),\n",
    "    epochs = 200,  # 어차피 early stopping이 적용될거라 epoch 커도 ok\n",
    "    batch_size = 32,  # 64보다, 128일때 속도가 더 빠르다...? 그런데, 32가 가장 일반적으로 사용하는 숫자라고 함\n",
    "    callbacks = callbacks,\n",
    "    verbose = 2,\n",
    "    shuffle = True) # True로 하더라도, 검증 데이터는 섞이지 않음 (True가 결과가 더 좋음)\n",
    "\n",
    "plotTrainingProgress(training_history=training_history_peakpower_case2_1, \n",
    "                     title=\"Case 2 ; peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 예측하기 (총전력)\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "test_predict_mergedInput_mergedOutput_forcingMax(model_peakpower_case2_1, \n",
    "             [XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             \"Case 2 : peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "model_peakpower_case2_1.evaluate([XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                               [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b7587",
   "metadata": {},
   "source": [
    "### PEAK : Case 2 (Trial 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d526c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_peakpower_case2_2 = build_transformer_conditionalInput_maxOutput_model_gAvgPooling(\n",
    "    input_shape = XHourlyPeakPowerTrain.shape[1:],\n",
    "    conditional_input_shape = XContinuousDailyPeakPowerTrain.shape[1:],\n",
    "    num_outputs = nForecastHours,\n",
    "    head_size=512,\n",
    "    num_heads=6,\n",
    "    ff_dim=6, # 8 : 성능이 오히려 떨어짐\n",
    "    num_transformer_blocks=6,\n",
    "    #mlp_units=[128, 256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_units=[512, 256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_dropout=0.1, # 0.4\n",
    "    dropout=0.1) # 0.25\n",
    "\"\"\"\n",
    "시계열 max가  예측 peak와 같아지는 것을 강제하는 가중치\n",
    "peak 예측도 중요하고, 시계열 예측도 중요(몇시에 발생하는지)하므로\n",
    "peak 예측에 대한 가중치를 높여나가면서 실험 함\n",
    "\"\"\"\n",
    "ts_max_weight = 0.1\n",
    "lossWeight = {\"ts_output\":1.0-ts_max_weight,\"ts_max\":ts_max_weight}\n",
    "\n",
    "# ts : time-series\n",
    "# ts_max의 영향을 줄이기 위해서 가중치 작게 설정했고, mae를 사용함\n",
    "# => mse로 변경\n",
    "# Adam + decay를 사용할거라면, 초기 lr은 큰 값으로 설정해도 되겠다.\n",
    "\"\"\"\n",
    "MyMetrics  라는 파일에 저장된 손실 함수를 사용하는 코드인데, 잘못된 구현이 있는것 같아서\n",
    "KERAS의 공식 손실 함수를 사용함\n",
    "model_peakpower_case2_2.compile(loss = {\"ts_output\" : 'mse', \"ts_max\" : 'mae'},\n",
    "                              optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                              metrics = {\"ts_output\":mae,\"ts_max\":mae},\n",
    "                              loss_weights = lossWeight)\n",
    "\"\"\"\n",
    "model_peakpower_case2_2.compile(loss = {\"ts_output\" : 'mean_squared_error', \"ts_max\" : 'mean_absolute_error'},\n",
    "                                optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                                metrics = {\"ts_output\" : 'mean_absolute_error',\"ts_max\" : 'mean_absolute_error'},\n",
    "                                loss_weights = lossWeight)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True)]\n",
    "\n",
    "training_history_peakpower_case2_2 = \\\n",
    "model_peakpower_case2_2.fit(\n",
    "    x = {\"ts_input\" : XHourlyPeakPowerTrain, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTrain},\n",
    "    y = {\"ts_output\" : yHourlyPeakPowerTrain, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTrain},    \n",
    "    validation_data=(\n",
    "        {\"ts_input\" : XHourlyPeakPowerTest, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTest},\n",
    "        {\"ts_output\" : yHourlyPeakPowerTest, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTest},        \n",
    "    ),\n",
    "    epochs = 200,  # 어차피 early stopping이 적용될거라 epoch 커도 ok\n",
    "    batch_size = 32,  # 64보다, 128일때 속도가 더 빠르다...? 그런데, 32가 가장 일반적으로 사용하는 숫자라고 함\n",
    "    callbacks = callbacks,\n",
    "    verbose = 2,\n",
    "    shuffle = True) # True로 하더라도, 검증 데이터는 섞이지 않음 (True가 결과가 더 좋음)\n",
    "\n",
    "plotTrainingProgress(training_history=training_history_peakpower_case2_2, \n",
    "                     title=\"Case 2 ; peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 예측하기 (총전력)\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "test_predict_mergedInput_mergedOutput_forcingMax(model_peakpower_case2_2, \n",
    "             [XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             \"Case 2 : peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "model_peakpower_case2_2.evaluate([XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                                 [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f180d9",
   "metadata": {},
   "source": [
    "### PEAK : Case 2 (Trial 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56207c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_peakpower_case2_3 = build_transformer_conditionalInput_maxOutput_model_gAvgPooling(\n",
    "    input_shape = XHourlyPeakPowerTrain.shape[1:],\n",
    "    conditional_input_shape = XContinuousDailyPeakPowerTrain.shape[1:],\n",
    "    num_outputs = nForecastHours,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4, # 8 : 성능이 오히려 떨어짐\n",
    "    num_transformer_blocks=4,\n",
    "    #mlp_units=[128, 256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_units=[256, 64], # 문제의 복잡도 증가로 인해, 디코더 쪽에서 MLP 계층을 늘렸음\n",
    "    mlp_dropout=0.1, # 0.4\n",
    "    dropout=0.1) # 0.25\n",
    "\"\"\"\n",
    "시계열 max가  예측 peak와 같아지는 것을 강제하는 가중치\n",
    "peak 예측도 중요하고, 시계열 예측도 중요(몇시에 발생하는지)하므로\n",
    "peak 예측에 대한 가중치를 높여나가면서 실험 함\n",
    "\"\"\"\n",
    "ts_max_weight = 0.1\n",
    "lossWeight = {\"ts_output\":1.0-ts_max_weight,\"ts_max\":ts_max_weight}\n",
    "\n",
    "# ts : time-series\n",
    "# ts_max의 영향을 줄이기 위해서 가중치 작게 설정했고, mae를 사용함\n",
    "# => mse로 변경\n",
    "# Adam + decay를 사용할거라면, 초기 lr은 큰 값으로 설정해도 되겠다.\n",
    "\"\"\"\n",
    "MyMetrics  라는 파일에 저장된 손실 함수를 사용하는 코드인데, 잘못된 구현이 있는것 같아서\n",
    "KERAS의 공식 손실 함수를 사용함\n",
    "model_peakpower_case2_3.compile(loss = {\"ts_output\" : 'mse', \"ts_max\" : 'mae'},\n",
    "                              optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                              metrics = {\"ts_output\":mae,\"ts_max\":mae},\n",
    "                              loss_weights = lossWeight)\n",
    "\"\"\"\n",
    "model_peakpower_case2_3.compile(loss = {\"ts_output\" : 'mean_squared_error', \"ts_max\" : 'mean_absolute_error'},\n",
    "                                optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                                metrics = {\"ts_output\" : 'mean_absolute_error',\"ts_max\" : 'mean_absolute_error'},\n",
    "                                loss_weights = lossWeight)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True)]\n",
    "\n",
    "training_history_peakpower_case2_3 = \\\n",
    "model_peakpower_case2_3.fit(\n",
    "    x = {\"ts_input\" : XHourlyPeakPowerTrain, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTrain},\n",
    "    y = {\"ts_output\" : yHourlyPeakPowerTrain, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTrain},    \n",
    "    validation_data=(\n",
    "        {\"ts_input\" : XHourlyPeakPowerTest, \n",
    "         \"ts_max_input\" : XContinuousDailyPeakPowerTest},\n",
    "        {\"ts_output\" : yHourlyPeakPowerTest, \n",
    "         \"ts_max\" : XContinuousDailyPeakPowerTest},        \n",
    "    ),\n",
    "    epochs = 200,  # 어차피 early stopping이 적용될거라 epoch 커도 ok\n",
    "    batch_size = 32,  # 64보다, 128일때 속도가 더 빠르다...? 그런데, 32가 가장 일반적으로 사용하는 숫자라고 함\n",
    "    callbacks = callbacks,\n",
    "    verbose = 2,\n",
    "    shuffle = True) # True로 하더라도, 검증 데이터는 섞이지 않음 (True가 결과가 더 좋음)\n",
    "\n",
    "plotTrainingProgress(training_history=training_history_peakpower_case2_3, \n",
    "                     title=\"Case 2 ; peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 예측하기 (총전력)\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "test_predict_mergedInput_mergedOutput_forcingMax(model_peakpower_case2_3, \n",
    "             [XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             \"Case 2 : peak power (%d LookBack)\"%(nLookBackDays))\n",
    "\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "model_peakpower_case2_3.evaluate([XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                                 [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d77f7b",
   "metadata": {},
   "source": [
    "### 3개 모델의 출력중에서 max 값을 취하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b936248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def test_predict_mergedInput_mergedOutput_forcingMax_multiplemodels(model1, model2, model3, Xmerged, ymerged, title, _ylim=None):\n",
    "    # X 데이터 분리\n",
    "    X = Xmerged[0]\n",
    "    conditionalX = Xmerged[1]\n",
    "    # y 데이터 분리\n",
    "    y = ymerged[0]\n",
    "    maxy = ymerged[1]\n",
    "\n",
    "    ts_pred = [] # 시계열 예측값을 저장할 리스트\n",
    "    actual = y.reshape(y.shape[0] * y.shape[1]) # 시계열 정답을 저장할 리스트\n",
    "    \n",
    "    ts_max_pred = [] # 24시간 예측 단위로, 예측으로 생성한 시계열 데이터의 max 저장할 리스트\n",
    "    max_y_values = [] # 24시간 예측 단위로, 정답에 해당하는 시계열 데이터의 max 저장할 리스트\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        x_sample1 = X[i].reshape(1, len(X[i]), 1)\n",
    "        x_sample2 = conditionalX[i].reshape(1, len(conditionalX[i]), 1)\n",
    "        \n",
    "        [y_hat1, y_hat_max1] = model1.predict([x_sample1, x_sample2])\n",
    "        [y_hat2, y_hat_max2] = model2.predict([x_sample1, x_sample2])\n",
    "        [y_hat3, y_hat_max3] = model3.predict([x_sample1, x_sample2])\n",
    "            \n",
    "        y_hat_values1 = y_hat1[0].reshape(len(y_hat1[0]),).tolist()\n",
    "        y_hat_values2 = y_hat2[0].reshape(len(y_hat2[0]),).tolist()\n",
    "        y_hat_values3 = y_hat3[0].reshape(len(y_hat3[0]),).tolist()\n",
    "        \n",
    "        y_hat_values = []\n",
    "        for j in range(len(y_hat_values1)):\n",
    "            y_hat_values.append(max(y_hat_values1[j],y_hat_values2[j],y_hat_values3[j]))\n",
    "\n",
    "        if len(ts_pred) == 0:\n",
    "            ts_pred = y_hat_values\n",
    "        else:\n",
    "            ts_pred = ts_pred + y_hat_values\n",
    "            \n",
    "        ts_max_pred.append(max(y_hat_max1,y_hat_max2,y_hat_max3))\n",
    "        max_y_values.append(maxy[i][0])\n",
    "            \n",
    "    # 시계열 데이터 예측값을 plot\n",
    "    print(\"TimeSeries MAE : %d\"%(int(mean_absolute_error(actual, ts_pred))))\n",
    "    #plt.figure(figsize=(20,10))\n",
    "    plt.figure()\n",
    "    plt.plot(actual, label='actual')\n",
    "    plt.plot(ts_pred, label='forecast')\n",
    "    plt.title(title) \n",
    "    plt.legend()\n",
    "    plt.xlabel('hours')\n",
    "    if _ylim is not None:\n",
    "        plt.ylim(_ylim)\n",
    "    plt.show()\n",
    "        \n",
    "    # 24시간 예측 단위로, 시계열 데이터의 총합을 plot\n",
    "    print(\"TimeSeriesSum MAE : %d\"%(int(mean_absolute_error(np.array(max_y_values), np.array(ts_max_pred)))))\n",
    "    #print(sum_y_values)\n",
    "    #print(ts_sum_pred)\n",
    "    plt.figure()\n",
    "    plt.bar(np.arange(len(max_y_values))-0.1, max_y_values, width=0.3, label='actual')\n",
    "    plt.bar(np.arange(len(ts_max_pred))+0.1, ts_max_pred, width=0.3, label='forecast')\n",
    "    plt.legend()\n",
    "    plt.title(\"Comparison: daily power max\")\n",
    "    plt.xlabel('days')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85733017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하기 (총전력)\n",
    "# 평가는 1-24시 단위로 자른 데이터로...\n",
    "test_predict_mergedInput_mergedOutput_forcingMax_multiplemodels(model_peakpower_case2_1, model_peakpower_case2_2, model_peakpower_case2_3, \n",
    "             [XHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             [yHourlyPeakPowerTest,XContinuousDailyPeakPowerTest], \n",
    "             \"Case 2 : peak power (%d LookBack)\"%(nLookBackDays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5855973",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_peakpower_case2_1.save('modelBsave/model1.h5')\n",
    "    model_peakpower_case2_2.save('modelBsave/model2.h5')\n",
    "    model_peakpower_case2_3.save('modelBsave/model3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
